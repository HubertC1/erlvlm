defaults:
    - _self_
    - agent: sac

algo_name: sac


# reward learning
segment: 1
activation: tanh
num_seed_steps: 1000
num_unsup_steps: 9000
num_interact: 5000
reward_lr: 0.0003
reward_batch: 50
reward_update: 10
feed_type: 0
reset_update: 100
topK: 5
ensemble_size: 3
max_feedback: 10000
large_batch: 10
label_margin: 0.0
teacher_beta: -1
teacher_gamma: 1
teacher_eps_mistake: 0
teacher_eps_skip: 0
teacher_eps_equal: 0

# scheduling
reward_schedule: 0

num_train_steps: 1000000
replay_buffer_capacity: ${num_train_steps}

# evaluation config
eval_frequency: 10000
num_eval_episodes: 10
device: cuda

# logger
log_frequency: 10000
log_save_wandb: true
save_interval: ${num_interact}

# video recorder
save_video: false

# wandb
run_group: ''
exp_name: ${run_group}_s${seed}
notes: ''
save_dir: './logs_metaworld/${env}'

# load pretrained models
reward_model_load_dir: "None"
agent_model_load_dir: "None"
model_load_step: 1000000

# debug
debug: false
mode: train
n_episodes_save_gif: 0

# setups
seed: 1

# Environment
env: dog_stand
gradient_update: 1

num_ratings: 2
softmax_temp: 30

# VLM teacher
vlm_feedback: True
image_reward: True
image_size: 224
resnet: 0 # default
conv_kernel_sizes: [5, 3, 3, 3] # default
conv_strides: [3, 2, 2, 2] # default
conv_n_channels: [16, 32, 64, 128] # default
n_processes_query: 1
reward_loss: "ce"
least_reward_update: 0
weighting_loss: False
batch_stratify: False
use_cached: False
query_cached: ''

# hydra configuration
hydra:
    output_subdir: null
    run:
        dir: ${save_dir}/${run_group}/${exp_name}